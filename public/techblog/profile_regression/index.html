<!DOCTYPE html>
<html lang="en-us">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta http-equiv="X-Clacks-Overhead" content="GNU Terry Pratchett" />
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title>Profile Regression vs other clustering methods on synthetic data | Bemsibom Toh</title>
<meta name="title" content="Profile Regression vs other clustering methods on synthetic data" />
<meta name="description" content="Introduction Profile regression is a clustering technique which links the clusters to an outcome of interest via a regression model. It differs from traditional distance-based clustering techniques in that it uses a stochastic model-based approach.
In general, models are fitted using Markov Chain Monte Carlo sampling methods, which outputs a different partition of the data at each iteration of the sampler.
The aim of this article is to compare the performance of profile regression to other clustering methods, such as Hierarchical clustering, K-Means, K-mode and Latent Class Analysis." />
<meta name="keywords" content="" />


<meta property="og:url" content="http://localhost:1313/techblog/profile_regression/">
  <meta property="og:site_name" content="Bemsibom Toh">
  <meta property="og:title" content="Profile Regression vs other clustering methods on synthetic data">
  <meta property="og:description" content="Introduction Profile regression is a clustering technique which links the clusters to an outcome of interest via a regression model. It differs from traditional distance-based clustering techniques in that it uses a stochastic model-based approach.
In general, models are fitted using Markov Chain Monte Carlo sampling methods, which outputs a different partition of the data at each iteration of the sampler.
The aim of this article is to compare the performance of profile regression to other clustering methods, such as Hierarchical clustering, K-Means, K-mode and Latent Class Analysis.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="techblog">
    <meta property="article:published_time" content="2020-12-16T00:00:00+00:00">
    <meta property="article:modified_time" content="2020-12-16T00:00:00+00:00">




  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Profile Regression vs other clustering methods on synthetic data">
  <meta name="twitter:description" content="Introduction Profile regression is a clustering technique which links the clusters to an outcome of interest via a regression model. It differs from traditional distance-based clustering techniques in that it uses a stochastic model-based approach.
In general, models are fitted using Markov Chain Monte Carlo sampling methods, which outputs a different partition of the data at each iteration of the sampler.
The aim of this article is to compare the performance of profile regression to other clustering methods, such as Hierarchical clustering, K-Means, K-mode and Latent Class Analysis.">




  <meta itemprop="name" content="Profile Regression vs other clustering methods on synthetic data">
  <meta itemprop="description" content="Introduction Profile regression is a clustering technique which links the clusters to an outcome of interest via a regression model. It differs from traditional distance-based clustering techniques in that it uses a stochastic model-based approach.
In general, models are fitted using Markov Chain Monte Carlo sampling methods, which outputs a different partition of the data at each iteration of the sampler.
The aim of this article is to compare the performance of profile regression to other clustering methods, such as Hierarchical clustering, K-Means, K-mode and Latent Class Analysis.">
  <meta itemprop="datePublished" content="2020-12-16T00:00:00+00:00">
  <meta itemprop="dateModified" content="2020-12-16T00:00:00+00:00">
  <meta itemprop="wordCount" content="1346">
<meta name="referrer" content="no-referrer-when-downgrade" />

  <style>
  body {
    font-family: Verdana, sans-serif;
    margin: auto;
    padding: 20px;
    max-width: 720px;
    text-align: left;
    background-color: #fff;
    word-wrap: break-word;
    overflow-wrap: break-word;
    line-height: 1.5;
    color: #444;
  }

  h1,
  h2,
  h3,
  h4,
  h5,
  h6,
  strong,
  b {
    color: #222;
  }

  a {
    color: #3273dc;
     
  }

  .title {
    text-decoration: none;
    border: 0;
  }

  .title span {
    font-weight: 400;
  }

  nav a {
    margin-right: 10px;
  }

  textarea {
    width: 100%;
    font-size: 16px;
  }

  input {
    font-size: 16px;
  }

  content {
    line-height: 1.6;
  }

  table {
    width: 100%;
  }

  img {
    max-width: 100%;
  }

  code {
    padding: 2px 5px;
    background-color: #f2f2f2;
  }

  pre code {
    color: #222;
    display: block;
    padding: 20px;
    white-space: pre-wrap;
    font-size: 14px;
    overflow-x: auto;
  }

  div.highlight pre {
    background-color: initial;
    color: initial;
  }

  div.highlight code {
    background-color: unset;
    color: unset;
  }

  blockquote {
    border-left: 1px solid #999;
    color: #222;
    padding-left: 20px;
    font-style: italic;
  }

  footer {
    padding: 25px;
    text-align: center;
  }

  .helptext {
    color: #777;
    font-size: small;
  }

  .errorlist {
    color: #eba613;
    font-size: small;
  }

   
  ul.blog-posts {
    list-style-type: none;
    padding: unset;
  }

  ul.blog-posts li {
    display: flex;
  }

  ul.blog-posts li span {
    flex: 0 0 130px;
  }

  ul.blog-posts li a:visited {
    color: #8b6fcb;
  }

  @media (prefers-color-scheme: dark) {
    body {
      background-color: #333;
      color: #ddd;
    }

    h1,
    h2,
    h3,
    h4,
    h5,
    h6,
    strong,
    b {
      color: #eee;
    }

    a {
      color: #8cc2dd;
    }

    code {
      background-color: #777;
    }

    pre code {
      color: #ddd;
    }

    blockquote {
      color: #ccc;
    }

    textarea,
    input {
      background-color: #252525;
      color: #ddd;
    }

    .helptext {
      color: #aaa;
    }
  }

</style>

</head>

<body>
  <header><a href="/" class="title">
  <h2>Bemsibom Toh</h2>
</a>
<nav><a href="/">Home</a>

<a href="/techblog/">technical blog</a>

<a href="/essays/">essays</a>

<a href="/"></a>


</nav>
</header>
  <main>

<content>
  <h1 id="introduction">Introduction</h1>
<p>Profile regression is a clustering technique which links the clusters to an outcome of interest via a regression model. It differs from traditional distance-based clustering techniques in that it uses a stochastic model-based approach.</p>
<p>In general, models are fitted using Markov Chain Monte Carlo sampling methods, which outputs a different partition of the data at each iteration of the sampler.</p>
<p>The aim of this article is to compare the performance of profile regression to other clustering methods, such as Hierarchical clustering, K-Means, K-mode and Latent Class Analysis. We will do this by applying profile regression to a set of 1000 simulated datasets, generated from code written by <a href="https://warwick.ac.uk/fac/sci/statistics/staff/academic-research/lnichols/">Linda Nichols</a> and <a href="https://research.birmingham.ac.uk/portal/en/persons/thomas-taverner(68156dc8-5825-4005-8f6c-559fcbf6717f).html">Tom Taverner</a>. The mathematical model used to generate these datasets will be the subject of another article.</p>
<p>We begin by loading the datasets and libraries we need as follows:</p>
<pre tabindex="0"><code>{r loading}
library(data.table)
library(ggplot2)
library(data.table)
library(ggplot2)
library(lpSolve)
library(viridisLite)
library(viridis)
library(hrbrthemes)
hrbrthemes::import_roboto_condensed()
library(mclust)
library(mcclust)
library(coda)
library(PReMiuM)
source(&#39;/Users/bemsi/Documents/BSU/code/transform_bham_data.R&#39;)
</code></pre><p>Next we load the different datasets we will use for this project:</p>
<pre tabindex="0"><code>{r LoadingData}
setwd(&#39;/Users/bemsi/Documents/BSU/code/&#39;)
birmingham.aris &lt;- get(load(&#39;/Users/bemsi/Documents/BSU/code/sim_20201009_ri.RData&#39;)) #ARIs from the other methods

#profile regression with no outcome
filelist = list.files(&#34;/Users/bemsi/Documents/BSU/code/SimulationOutputNoOutcome&#34;, pattern = &#34;\\pear_aris.txt$&#34;, full.names=TRUE) 
premium.aris.no.outcome &lt;- rbindlist(sapply(filelist, fread, simplify=FALSE, USE.NAMES = TRUE))

#profile regression with random outcome
filelist = list.files(&#34;/Users/bemsi/Documents/BSU/code/SimulationOutputRandomOutcome&#34;, pattern = &#34;\\pear_aris.txt$&#34;, full.names=TRUE)
premium.aris.random.outcome &lt;- rbindlist(sapply(filelist, fread, simplify=FALSE, USE.NAMES = TRUE))

#profile regression with deterministic outcome
filelist = list.files(&#34;/Users/bemsi/Documents/BSU/code/SimulationOutputDetOutcome&#34;, pattern = &#34;\\pear_aris.txt$&#34;, full.names=TRUE)
premium.aris.det.outcome &lt;- rbindlist(sapply(filelist, fread, simplify=FALSE, USE.NAMES = TRUE))

#profile regression with semi-deterministic outcome 1
filelist = list.files(&#34;/Users/bemsi/Documents/BSU/code/DetOutcomeOne&#34;, pattern = &#34;\\pear_aris.txt$&#34;, full.names=TRUE)
premium.aris.det.outcome.1 &lt;- rbindlist(sapply(filelist, fread, simplify=FALSE, USE.NAMES = TRUE))

#profile regression with semi-deterministic outcome 2
filelist = list.files(&#34;/Users/bemsi/Documents/BSU/code/DetOutcomeThree&#34;, pattern = &#34;\\pear_aris.txt$&#34;, full.names=TRUE)
premium.aris.det.outcome.2 &lt;- rbindlist(sapply(filelist, fread, simplify=FALSE, USE.NAMES = TRUE))

# Load and transform the first dataset in the bank of simulated datasets
birmingham.dfs &lt;- get(load(&#39;/Users/bemsi/Documents/BSU/code/sim_20201009_df.rData&#39;))
sample.dataframe &lt;- birmingham.dfs[[1]]
inputs &lt;- transform_bham_data.one(sample.dataframe)

#modifying original dataframe
birmingham.aris$PRegr &lt;- premium.aris.no.outcome$V1
birmingham.aris.one &lt;- reshape2::melt(birmingham.aris , measure.vars=c(&#34;LCA&#34;, &#34;kmode&#34;, &#34;HCA&#34;, &#34;MCAkm&#34;, &#34;kmean&#34;, &#34;PRegr&#34;))
names(birmingham.aris.one) &lt;- c(&#34;Method&#34;, &#34;ARI&#34;)
</code></pre><p>We now visualize the data, to see whether there is any apparent clustering structure in it.</p>
<pre tabindex="0"><code>{r visualizeData}
clusters &lt;- data.frame(inputs$inputData$group)
colnames(clusters) &lt;- &#34;Cluster&#34;
pheatmap::pheatmap(inputs$inputData[2:27],  
                   cluster_rows = F,
                   cluster_cols = F,
                   show_rownames = FALSE,
                   legend = FALSE,
                   cutree_rows = 4, 
                   col = c(&#34;white&#34;, &#34;black&#34;),
                   annotation_row = clusters)
</code></pre><p>We can see that there is some structure in the dataset, and there are three clearly identifiable clusters in the dataset, as well as one &rsquo;noisy&rsquo; cluster.</p>
<h1 id="no-outcome">No outcome</h1>
<p>We begin by looking at the output of profile regression when there is no supervision - i.e., we carry out profile regression with no outcome to guide the clustering algorithm.</p>
<h2 id="convergence-analysis">Convergence analysis</h2>
<p>We will use three methods to assess convergence of the profile regression algorithm: a visual method using traceplots of certain parameters, the <a href="https://www.rdocumentation.org/packages/coda/versions/0.19-4/topics/gelman.diag">Gelman Diagnostic</a>, and the <a href="https://www.rdocumentation.org/packages/coda/versions/0.19-4/topics/geweke.diag">Geweke Diagnostic</a>.</p>
<p>We begin by plotting the values of <code>alpha</code> through the different steps of the MCMC.</p>
<pre tabindex="0"><code>{r plotAlphas}
birmingham.dataframes &lt;- get(load(&#39;~/Documents/BSU/code/sim_20201009_df.RData&#39;))
sample.dataframe &lt;- birmingham.dataframes[[1]] #We are taking the first dataframe in our series
group.labels &lt;- sample.dataframe$group
alphas &lt;- fread(&#34;~/Documents/BSU/code/MCMCDiagnostics/myOutput_1_alpha.txt&#34;)
plot(alphas$V1, type=&#39;l&#39;, col=&#39;red&#39;, ylab=&#39;alpha&#39;, xlab=&#39;&#39;)
</code></pre><p>We can that the algorithm converges after several iterations.</p>
<p>We now run the Geweke diagnostic test on the alphas to see if there is convergence.</p>
<pre tabindex="0"><code>{r geweke}
mcmc_alpha &lt;- mcmc(data=alphas$V1)
geweke.diag(mcmc_alpha)
</code></pre><p>We obtain a z-value of about 1.57, which is within the acceptance region for a significance level of 0.05.</p>
<p>To run the Gelman diagnostic, we need two chains. So we run Premium twice on the same dataset, using different starting points (seeds), and then proceed to carry out this test.</p>
<pre tabindex="0"><code>{r GelmanDiagnostic}
# runInfoObj &lt;- profRegr(yModel = inputs$yModel,
#                        xModel = inputs$xModel,
#                        nSweeps = 30000,
#                        nBurn = 2000,
#                        data = inputs$inputData[,2:27],
#                        output=&#34;myOutput_unsupervised_gelman_1&#34;,
#                        covNames = names(inputs$inputData[,2:27]),
#                        reportBurnIn = TRUE,
#                        seed = 12,
#                        excludeY = TRUE)
# 
# runInfoObj &lt;- profRegr(yModel = inputs$yModel,
#                        xModel = inputs$xModel,
#                        nSweeps = 30000,
#                        nBurn = 2000,
#                        data = inputs$inputData[,2:27],
#                        output=&#34;myOutput_unsupervised_gelman_2&#34;,
#                        covNames = names(inputs$inputData[,2:27]),
#                        reportBurnIn = TRUE,
#                        seed = 13,
#                        excludeY = TRUE)

alpha_1 &lt;- fread(&#34;~/Documents/BSU/code/myOutput_unsupervised_gelman_1_alpha.txt&#34;)
alpha_2 &lt;- fread(&#34;~/Documents/BSU/code/myOutput_unsupervised_gelman_2_alpha.txt&#34;)
mcmc_alpha_1 &lt;- mcmc(data=alpha_1$V1)
mcmc_alpha_2 &lt;- mcmc(data=alpha_2$V1)
combined.chains &lt;- mcmc.list(mcmc_alpha_1,mcmc_alpha_2)
gelman.diag(combined.chains)
</code></pre><p>The Gelman diagnostic here is 1. A factor of 1 means that between variance and within chain variance are equal, larger values mean that there is still a notable difference between chains.</p>
<p>Now we visualize the distribution of ARIs for the different methods when applied on the dataset.</p>
<pre tabindex="0"><code>{r firstplot}
ggplot(birmingham.aris.one, aes(x=Method, y=ARI, fill=Method)) +
  geom_boxplot() +
  scale_fill_viridis(discrete = TRUE, alpha=0.6) +
  geom_jitter(color=&#34;black&#34;, size=0.4, alpha=0.9) +
  theme_ipsum() +
  theme(
    legend.position=&#34;none&#34;,
    plot.title = element_text(size=11)
  ) +
  ggtitle(&#34;Adjusted Rand Indices for different methods&#34;) +
  xlab(&#34;Methods&#34;)
</code></pre><p>The ARIs are quite low in general. A possible reason for this is that there may be empty rows in the dataset which need to be eliminated.</p>
<p>Unsupervised profile regression does&hellip; okay, as you can see from the boxplots. Given that the number of clusters present in the dataset is known to the other clustering algorithms, but not to profile regression, this is not entirely a bad situation.</p>
<h2 id="outcome---guided-profile-regression">Outcome - guided profile regression</h2>
<p>Now, we want to investigate the effect of incorporating an outcome we incorporate an outcome to the dataset randomly. We now want to improve on the clarity of the outcome used in the analysis. &lsquo;Clarity&rsquo; in this scenario is a measure of correlation between the binary data and the outcome. We will experiment with discrete, binary outcomes in the first instance, and consider different levels of clarity. To do this, we will control the probability of the outcome being equal to 1 for a given patient - the closer this probability is to 1, the clearer the outcome is. This is done as follows:</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>And then the <code>outcome</code> vector is added on to the dataset, and then analysed accordingly.</p>
<p>Using this code, we can derive four different outcomes:</p>
<ul>
<li><em>Random outcome</em>: The probabilities are <code>c(0.5, 0.5)</code></li>
<li><em>Deterministic outcome</em>: The probabilities are <code>c(1,0)</code></li>
<li><em>Semi-deterministic outcome 1</em>: he probabilities are <code>c(0.6,0.4)</code></li>
<li><em>Semi-deterministic outcome 2</em>: he probabilities are <code>c(0.8,0.2)</code></li>
</ul>
<p>We now run <code>PReMiuM</code> on datasets with these outcomes incorporated, and we get the following results:</p>
<pre tabindex="0"><code>birmingham.aris$NoOutcome &lt;- premium.aris.no.outcome$V1
birmingham.aris$RandomOutcome &lt;- premium.aris.random.outcome$V1
birmingham.aris$DetOutcome &lt;- premium.aris.det.outcome$V1
birmingham.aris$SemiDetOne &lt;- premium.aris.det.outcome.1$V1
birmingham.aris$SemiDetTwo &lt;- premium.aris.det.outcome.2$V1

interesting.columns &lt;- c(&#39;HCA&#39;, &#39;PRegr&#39;, &#39;RandomOutcome&#39;, &#39;SemiDetOne&#39;, &#39;SemiDetTwo&#39;, &#39;DetOutcome&#39;)
 
birmingham.aris.two &lt;- reshape2::melt(birmingham.aris[interesting.columns], measure.vars=interesting.columns)
names(birmingham.aris.two) &lt;- c(&#34;Method&#34;, &#34;ARI&#34;)

ggplot(birmingham.aris.two, aes(x=Method, y=ARI, fill=Method)) +
  geom_boxplot() +
  scale_fill_viridis(discrete = TRUE, alpha=0.6) +
  geom_jitter(color=&#34;black&#34;, size=0.4, alpha=0.9) +
  theme_ipsum() +
  theme(
    legend.position=&#34;none&#34;,
    plot.title = element_text(size=11)
  ) +
  ggtitle(&#34;Adjusted Rand Indices for different methods&#34;) +
  xlab(&#34;Methods&#34;)
</code></pre><p>We can see from there that a random outcome (which has no discernible relationship with the rest of the dataset) performs worse than no outcome at all. But once there is some structure in the outcome, the performance of Profile Regression improves quite drastically.</p>
<h2 id="same-analysis-this-time-with-a-cleaner-dataset">Same analysis, this time with a cleaner dataset</h2>
<p>This time, we repeat the analysis for PReMiuM, but we remove all the people who are positive for none of the diseases - i.e., empty rows in the dataset. We want to see whether there will be an improvement in the ARIs.</p>
<pre tabindex="0"><code>#profile regression with no outcome
filelist = list.files(&#34;/Users/bemsi/Documents/BSU/code/NewDetOutcome1/SimulationOutput&#34;, pattern = &#34;\\pear_aris.txt$&#34;, full.names=TRUE) 
clean.premium.no.outcome &lt;- rbindlist(sapply(filelist, fread, simplify=FALSE, USE.NAMES = TRUE))

#profile regression with random outcome
filelist = list.files(&#34;/Users/bemsi/Documents/BSU/code/NewDetOutcome2/SimulationOutput&#34;, pattern = &#34;\\pear_aris.txt$&#34;, full.names=TRUE)
clean.premium.det.outcome.two &lt;- rbindlist(sapply(filelist, fread, simplify=FALSE, USE.NAMES = TRUE))

#profile regression with deterministic outcome
filelist = list.files(&#34;/Users/bemsi/Documents/BSU/code/NewDetOutcome3/SimulationOutput&#34;, pattern = &#34;\\pear_aris.txt$&#34;, full.names=TRUE)
clean.premium.det.outcome.three &lt;- rbindlist(sapply(filelist, fread, simplify=FALSE, USE.NAMES = TRUE))

#profile regression with semi-deterministic outcome 1
filelist = list.files(&#34;/Users/bemsi/Documents/BSU/code/NewDetOutcome4/SimulationOutput&#34;, pattern = &#34;\\pear_aris.txt$&#34;, full.names=TRUE)
clean.premium.det.outcome &lt;- rbindlist(sapply(filelist, fread, simplify=FALSE, USE.NAMES = TRUE))

inputs.cleaned &lt;- transform_bham_data(sample.dataframe)

cleaned.premium &lt;- data.frame(clean.premium.no.outcome, clean.premium.det.outcome.two, clean.premium.det.outcome.three, clean.premium.det.outcome.three)
colnames(cleaned.premium) &lt;- c(&#34;NoOutcomes&#34;, &#34;SemiDetOne&#34;, &#34;SemiDetTwo&#34;, &#34;DetOutcome&#34;)

new.aris &lt;- reshape2::melt(cleaned.premium, measure.vars=colnames(cleaned.premium))
names(new.aris) &lt;- c(&#34;Method&#34;, &#34;ARI&#34;)

ggplot(new.aris, aes(x=Method, y=ARI, fill=Method)) +
  geom_boxplot() +
  scale_fill_viridis(discrete = TRUE, alpha=0.6) +
  geom_jitter(color=&#34;black&#34;, size=0.4, alpha=0.9) +
  theme_ipsum() +
  theme(
    legend.position=&#34;none&#34;,
    plot.title = element_text(size=11)
  ) +
  ggtitle(&#34;Adjusted Rand Indices for cleaned dataset&#34;) +
  xlab(&#34;Methods&#34;)
</code></pre><p>We see that we get much stronger ARIs, after eliminating the empty rows. The patterns are quite similar to those with the more messy data - the clearer the outcome, the better the ability of the algorithm to recover the clustering structure.</p>

</content>
<p>
  
</p>

  </main>
  <footer>Made with <a href="https://github.com/janraasch/hugo-bearblog/">Hugo ʕ•ᴥ•ʔ Bear</a>
</footer>

    
</body>

</html>
